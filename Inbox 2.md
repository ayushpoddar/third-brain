- A rate limiter can either be at the client or the server side
	- Client-side implementation
		- Client is an unreliable place to enforce rate limiting because client requests can be forged by malicious actors.
![Client side rate limiter](Server%20side%20rate%20limiter%20demonstration.webp)

- As an alternative, we can also create a rate limiter middleware.
- ![Rate limiter middleware demo](Assets/Rate_limiter_middleware_demo.webp)
- Due to the popularity of cloud microservices, rate limiting is usually implemented within a component called API gateway
- If your system already uses microservice architecture and includes an API gateway to perform authentication, IP whitelisting, etc, it is better to add the rate limiter to the API gateway instead of implementing it in the server
- Token bucket algorithm
	- A token bucket is a container that has pre-defined capacity.
	- Tokens are put in the bucket at preset rates periodically.
	- Once the bucket is full, no more tokens are added.
	- Each request consumes one token. When a request arrives, we check if there are enough tokens in the bucket.
	- If there are enough tokens, we take one token out for each request, and the request goes through.
	- If there are not enough tokens, the request is dropped.
	- Example: ![4 tokens filled every minute](Assets/4_tokens_filled_every_minute.svg)
	- The number of buckets can be decided on the requirements. One bucket means one rate limiting rule. For example, if you want to throttle based on IP addresses, we require one bucket per IP address
	- Pros
		- Easy to implement
		- Memory efficient
		- Allows a burst of traffic for short periods
	- Cons
		- Challenging to tune the bucket size and token refill rate
- Leaking bucket algorithm
	- Similar to the token bucket algorithm, except that the requests are processed at a fixed rate.
	- Implemented with a first-in-first-out (FIFO) queue
	- How it works? ![Leaking bucket algorithm working](Assets/Leaking_bucket_algorithm_working.svg)
	- Pros:
		- Memory efficient due to the limited queue size
		- Suitable for cases where stable processing rate is desired
	- Cons
		- Not suitable for a burst of traffic
		- Difficult to tune the two parameters: Bucket size, Outflow rate
- Fixed window counter algorithm
	- Divide the timeline into fix-sized time windows.
	- Assign a counter for each window. Counter is reset at the start of each window
	- Each requests increments the counter by one
	- After the counter reaches the threshold, new requests are dropped
	- Cons
		- Spike in traffic at the edges of a window can cause more requests than the allowed quota to go through
		- ![Demonstrating bursts of traffic breaking the fixed window counter rate limiter](Assets/Demonstrating_bursts_of_traffic_breaking_the_fixed_window_counter_rate_limiter.svg)
			- The system allows a maximum of 5 requests per minutes, and the counter resets at the human-friendly round minute
			- There are 5 requests just before 2:01, and 5 requests just after 2:01.
			- This leads to 10 requests going through around the 2:01 mark
	- Pros
		- Memory efficient
		- Simple and easy
- Sliding window log algorithm
	- How it works (assuming a limit of 5 requests per minute)
		- Every time we receive a request, we check if we have received 5 or more requests in the last 1 minute
		- When a request is received, its timestamp is logged. This logging of timestamp helps us count the number of requests received in the last minute
		- ![[Pasted image 20230609085817.png|Sliding window log algorithm]]
		- Pros
			- Accurate rate limiting due to the rolling window
		- Cons
			- Memory intensive
- Sliding window counter algorithm
	- Combination of fixed window counter and sliding window log algorithms
	- How it works?
		- Assume that 5 requests are allowed per minute
		- There were 5 requests made in the previous minute
		- A request arrives at 15th second (25% of the current minute) of the current minute
		- Then, the number of requests in the rolling window is calculated using:
			- Requests in current window + requests in the previous window * overlap percentage of the rolling window and the previous window
			- 0 + 5 * 0.75
			- 3.75 - This can be rounded up or down, depending on the use case. Lets round up to 4
		- Since 4 is less than 5, this request (at the 15th second) will be allowed to pass through
		- ![Sliding window counter algorithm demo](Assets/Sliding_window_counter_algorithm_demo.svg)
	- Pros
		- Smooths out spikes in traffic
		- Memory efficient
	- Cons
		- It is an approximation of the actual rate because it assumes requests in the previous window are evenly distributed.
			- This problem is not as bad as it seems. [Experiments done by Cloudflare](https://blog.cloudflare.com/counting-things-a-lot-of-different-things/), only 0.003% of requests are wrongly allowed or rate limited among 400 million requests
- Storing these counters
	- Using database is not a good idea due to slowness of disk access.
	- In-memory cache makes sense because it is fast and supports time-based expiration strategy. Eg: Redis
- ![High level rate limiter architecture](Assets/High_level_rate_limiter_architecture.webp)
- What happens when requests are rate limited?
	- APIs return a HTTP response code - 429 (too many requests)
	- We may choose to enqueue rate-limited requests to be processed later
- HTTP headers in a 429 response
	- `X-Ratelimit-Remaining`: The remaining number of allowed requests within the window. 
	- `X-Ratelimit-Limit`: It indicates how many calls the client can make per time window. 
	- `X-Ratelimit-Retry-After`: The number of seconds to wait until you can make a request again without being throttled.
- Challenges in scaling the rate limiter to support multiple servers and concurrent threads
	- Race conditions
		- ![Rate limiter race condition](Assets/Rate_limiter_race_condition.svg)
	- Synchronization issue
		- We need multiple rate limiters to handle millions of users
		- ![Rate limiter synchronisation issue](Assets/rate_limiter_synchronisation_issue.png)
			- Here, both the rate limiters must share the counters for both clients, otherwise the rate limiters would fail their purpose
		- In such cases, we need synchronization between rate limiters to keep the counters in sync
		- One possible solution is to use centralized data stores like Redis. ![Centralized redis rate limiters](Assets/centralized_redis_rate_limiters.png)
- Rate limiting can be different levels. We can also apply rate limiting by IP addresses using Iptables.